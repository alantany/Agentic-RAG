import streamlit as st
import importlib
import subprocess
import sys
import sqlite3
import json
from openai import OpenAI
import networkx as nx
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import pickle
import os
import pdfplumber
from datetime import datetime
import re

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(page_title="åŒ»ç–— RAG ç³»ç»Ÿ", layout="wide")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'data_initialized' not in st.session_state:
    st.session_state.data_initialized = False

# å‘é‡å­˜å‚¨ç±»
class VectorStore:
    def __init__(self):
        self.model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        self.dimension = 384
        self.index = faiss.IndexFlatL2(self.dimension)
        self.documents = []
        if os.path.exists('vector_store.pkl'):
            self.load_store()
    
    def text_to_vector(self, text: str):
        return self.model.encode(text)
    
    def add_document(self, doc: dict):
        vector = self.text_to_vector(doc["content"])
        self.index.add(np.array([vector], dtype=np.float32))
        self.documents.append(doc)
        self.save_store()
    
    def search(self, query: str, limit: int = 3):
        query_vector = self.text_to_vector(query)
        D, I = self.index.search(np.array([query_vector], dtype=np.float32), limit)
        results = []
        for idx in I[0]:
            if idx < len(self.documents):
                results.append(self.documents[idx])
        return results
    
    def save_store(self):
        store_data = {
            'index': faiss.serialize_index(self.index),
            'documents': self.documents
        }
        with open('vector_store.pkl', 'wb') as f:
            pickle.dump(store_data, f)
    
    def load_store(self):
        try:
            with open('vector_store.pkl', 'rb') as f:
                store_data = pickle.load(f)
                self.index = faiss.deserialize_index(store_data['index'])
                self.documents = store_data['documents']
        except Exception as e:
            print(f"åŠ è½½å‘é‡å­˜å‚¨å¤±è´¥: {str(e)}")

# PDFè§£æç±»
class MedicalRecordParser:
    def __init__(self, pdf_content):
        self.content = pdf_content
        self.parsed_data = self._parse_content()
    
    def _parse_content(self):
        data = {}
        try:
            # æå–åŸºæœ¬ä¿¡æ¯
            data['name'] = re.search(r'å§“å\s*([\u4e00-\u9fa5]+)', self.content).group(1)
            data['gender'] = re.search(r'æ€§åˆ«\s*([\u4e00-\u9fa5]+)', self.content).group(1)
            data['age'] = int(re.search(r'å¹´é¾„\s*(\d+)å²', self.content).group(1))
            data['ethnicity'] = re.search(r'æ°‘æ—\s*([\u4e00-\u9fa5]+)', self.content).group(1)
            data['marriage'] = re.search(r'å©šå§»\s*([\u4e00-\u9fa5]+)', self.content).group(1)
            
            # æå–æ—¥æœŸ
            admission_date = re.search(r'ä½é™¢æ—¥æœŸ\s*:(\d{4}\s*å¹´\d{1,2}æœˆ\d{1,2}æ—¥)', self.content).group(1)
            data['admission_date'] = datetime.strptime(admission_date.replace(' ', ''), '%Yå¹´%mæœˆ%dæ—¥').strftime('%Y-%m-%d')
            
            # æå–è¯Šæ–­
            diagnoses = []
            if match := re.search(r'å‡ºé™¢è¯Šæ–­\s*:(.*?)å‡ºé™¢æ—¶æƒ…å†µ', self.content, re.DOTALL):
                diagnoses_text = match.group(1)
                diagnoses = [d.strip() for d in diagnoses_text.split('\n') if d.strip()]
            data['diagnoses'] = diagnoses
            
            # æå–ç—‡çŠ¶
            symptoms = []
            if match := re.search(r'ä¸»\s*è¯‰\s*:(.*?)å…¥é™¢æ—¶æƒ…å†µ', self.content, re.DOTALL):
                symptoms_text = match.group(1)
                symptoms = [s.strip() for s in re.findall(r'[ï¼Œã€‚ã€](.*?)[ï¼Œã€‚ã€]', symptoms_text)]
            data['symptoms'] = symptoms
            
            # æå–æ£€æŸ¥ç»“æœ
            examinations = {}
            exam_patterns = {
                'å¤´é¢…MRI': r'å¤´é¢…\s*MRI\s*æç¤º(.*?)ã€‚',
                'åŠ¨æ€å¿ƒç”µå›¾': r'åŠ¨æ€å¿ƒç”µå›¾\s*:(.*?)ã€‚',
                'çœ¼éœ‡ç”µå›¾': r'çœ¼éœ‡ç”µå›¾æç¤º(.*?)ã€‚'
            }
            for exam, pattern in exam_patterns.items():
                if match := re.search(pattern, self.content):
                    examinations[exam] = match.group(1).strip()
            data['examinations'] = examinations
            
            return data
        except Exception as e:
            st.error(f"è§£æPDFå†…å®¹é”™è¯¯: {str(e)}")
            return {}

# æ•°æ®å¯¼å…¥å‡½æ•°
def import_medical_data(pdf_content):
    parser = MedicalRecordParser(pdf_content)
    
    # å¯¼å…¥å‘é‡æ•°æ®åº“
    vector_store = VectorStore()
    document = {
        "title": f"{parser.parsed_data['name']}çš„ç—…å†",
        "content": pdf_content,
        "type": "ç—…å†è®°å½•",
        "date": parser.parsed_data['admission_date']
    }
    vector_store.add_document(document)
    
    # å¯¼å…¥å…³ç³»æ•°æ®åº“
    conn = sqlite3.connect('medical_records.db')
    cursor = conn.cursor()
    
    # åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS patients (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        name TEXT, gender TEXT, age INTEGER,
        ethnicity TEXT, marriage TEXT,
        admission_date DATE, discharge_date DATE
    )
    ''')
    
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS diagnoses (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        patient_id INTEGER, diagnosis TEXT,
        diagnosis_type TEXT,
        FOREIGN KEY (patient_id) REFERENCES patients(id)
    )
    ''')
    
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS examinations (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        patient_id INTEGER, exam_type TEXT,
        exam_result TEXT, exam_date DATE,
        FOREIGN KEY (patient_id) REFERENCES patients(id)
    )
    ''')
    
    # æ’å…¥æ•°æ®
    cursor.execute('''
    INSERT INTO patients (name, gender, age, ethnicity, marriage, admission_date)
    VALUES (?, ?, ?, ?, ?, ?)
    ''', (
        parser.parsed_data['name'],
        parser.parsed_data['gender'],
        parser.parsed_data['age'],
        parser.parsed_data['ethnicity'],
        parser.parsed_data['marriage'],
        parser.parsed_data['admission_date']
    ))
    
    patient_id = cursor.lastrowid
    
    for diagnosis in parser.parsed_data['diagnoses']:
        cursor.execute('''
        INSERT INTO diagnoses (patient_id, diagnosis, diagnosis_type)
        VALUES (?, ?, ?)
        ''', (patient_id, diagnosis, 'å‡ºé™¢è¯Šæ–­'))
    
    for exam_type, result in parser.parsed_data['examinations'].items():
        cursor.execute('''
        INSERT INTO examinations (patient_id, exam_type, exam_result, exam_date)
        VALUES (?, ?, ?, ?)
        ''', (patient_id, exam_type, result, parser.parsed_data['admission_date']))
    
    conn.commit()
    conn.close()
    
    # å¯¼å…¥å›¾æ•°æ®åº“
    G = nx.Graph()
    
    # æ·»åŠ æ‚£è€…èŠ‚ç‚¹
    G.add_node(parser.parsed_data['name'],
               type="patient",
               age=parser.parsed_data['age'],
               gender=parser.parsed_data['gender'])
    
    # æ·»åŠ è¯Šæ–­èŠ‚ç‚¹å’Œå…³ç³»
    for diagnosis in parser.parsed_data['diagnoses']:
        G.add_node(diagnosis, type="diagnosis")
        G.add_edge(parser.parsed_data['name'], diagnosis, relationship="diagnosed_with")
    
    # æ·»åŠ ç—‡çŠ¶èŠ‚ç‚¹å’Œå…³ç³»
    for symptom in parser.parsed_data['symptoms']:
        G.add_node(symptom, type="symptom")
        G.add_edge(parser.parsed_data['name'], symptom, relationship="has_symptom")
    
    # æ·»åŠ æ£€æŸ¥ç»“æœèŠ‚ç‚¹å’Œå…³ç³»
    for exam_type, result in parser.parsed_data['examinations'].items():
        G.add_node(exam_type, type="examination", result=result)
        G.add_edge(parser.parsed_data['name'], exam_type, relationship="underwent")
    
    nx.write_gexf(G, "medical_graph.gexf")
    
    return True

# æœç´¢å‡½æ•°
def get_vector_search_results(query: str) -> list:
    try:
        vector_store = VectorStore()
        results = vector_store.search(query)
        return [f"{item['title']}: {item['content']}" for item in results]
    except Exception as e:
        st.error(f"å‘é‡æœç´¢é”™è¯¯: {str(e)}")
        return []

def get_rdb_search_results(query: str) -> list:
    try:
        conn = sqlite3.connect('medical_records.db')
        cursor = conn.cursor()
        cursor.execute("""
            SELECT p.name, d.diagnosis 
            FROM patients p 
            JOIN diagnoses d ON p.id = d.patient_id 
            WHERE d.diagnosis LIKE ?
        """, (f"%{query}%",))
        results = cursor.fetchall()
        conn.close()
        return [f"{name}: {diagnosis}" for name, diagnosis in results]
    except Exception as e:
        st.error(f"æ•°æ®åº“æœç´¢é”™è¯¯: {str(e)}")
        return []

def get_graph_search_results(query: str) -> list:
    try:
        G = nx.read_gexf("medical_graph.gexf")
        results = []
        for node in G.nodes(data=True):
            if query.lower() in str(node[1]).lower():
                neighbors = list(G.neighbors(node[0]))
                results.append(f"{node[0]} ç›¸å…³: {neighbors}")
        return results[:3]
    except Exception as e:
        st.error(f"å›¾æ•°æ®åº“æœç´¢é”™è¯¯: {str(e)}")
        return []

def get_llm_response(query: str, search_results: dict) -> str:
    try:
        client = OpenAI(
            api_key="sk-2D0EZSwcWUcD4c2K59353b7214854bBd8f35Ac131564EfBa",
            base_url="https://free.gpt.ge/v1"
        )
        
        prompt = f"""
        æŸ¥è¯¢: {query}
        æœç´¢ç»“æœ:
        å‘é‡æœç´¢: {search_results['vector']}
        å…³ç³»æ•°æ®åº“: {search_results['rdb']}
        å›¾æ•°æ®åº“: {search_results['graph']}
        
        è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ç”Ÿæˆå›ç­”ã€‚
        """
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        st.error(f"LLMå“åº”é”™è¯¯: {str(e)}")
        return "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚"

# ä¸»ç•Œé¢
st.title("ğŸ¥ åŒ»ç–— RAG ç³»ç»Ÿ")

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.header("ç³»ç»Ÿé…ç½®")
    
    # æ•°æ®å¯¼å…¥éƒ¨åˆ†
    st.subheader("æ•°æ®å¯¼å…¥")
    uploaded_file = st.file_uploader("ä¸Šä¼ ç—…å†PDFæ–‡ä»¶", type=['pdf'])
    
    if uploaded_file is not None:
        if st.button("å¯¼å…¥æ•°æ®"):
            with st.spinner("æ­£åœ¨å¯¼å…¥æ•°æ®..."):
                try:
                    # ä½¿ç”¨pdfplumberè¯»å–PDFå†…å®¹
                    with pdfplumber.open(uploaded_file) as pdf:
                        pdf_content = ""
                        for page in pdf.pages:
                            pdf_content += page.extract_text()
                    
                    # å¯¼å…¥æ•°æ®
                    if import_medical_data(pdf_content):
                        st.success("æ•°æ®å¯¼å…¥æˆåŠŸï¼")
                        st.session_state.data_initialized = True
                    else:
                        st.error("æ•°æ®å¯¼å…¥å¤±è´¥ï¼")
                except Exception as e:
                    st.error(f"PDFè¯»å–é”™è¯¯: {str(e)}")
    
    # æœç´¢æ–¹æ³•é€‰æ‹©
    st.subheader("æœç´¢é…ç½®")
    search_methods = st.multiselect(
        "é€‰æ‹©æœç´¢æ–¹æ³•",
        ["å‘é‡æœç´¢", "å…³ç³»æ•°æ®åº“", "å›¾æ•°æ®åº“"],
        default=["å‘é‡æœç´¢", "å…³ç³»æ•°æ®åº“", "å›¾æ•°æ®åº“"]
    )

# ä¸»è¦å†…å®¹åŒºåŸŸ
query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š")

if st.button("æœç´¢å¹¶ç”Ÿæˆ"):
    if not st.session_state.data_initialized:
        st.warning("è¯·å…ˆå¯¼å…¥æ•°æ®ï¼")
    else:
        with st.spinner("æ­£åœ¨å¤„ç†..."):
            # æ‰§è¡Œæœç´¢
            search_results = {
                "vector": get_vector_search_results(query) if "å‘é‡æœç´¢" in search_methods else [],
                "rdb": get_rdb_search_results(query) if "å…³ç³»æ•°æ®åº“" in search_methods else [],
                "graph": get_graph_search_results(query) if "å›¾æ•°æ®åº“" in search_methods else []
            }
            
            # è·å–LLMå“åº”
            response = get_llm_response(query, search_results)
            
            # æ›´æ–°å¯¹è¯å†å²
            st.session_state.chat_history.append({"query": query, "response": response})

# æ˜¾ç¤ºå¯¹è¯å†å²
st.subheader("å¯¹è¯å†å²")
for chat in st.session_state.chat_history:
    st.text_area("é—®é¢˜ï¼š", chat["query"], height=50, disabled=True)
    st.text_area("å›ç­”ï¼š", chat["response"], height=100, disabled=True)
    st.markdown("---")