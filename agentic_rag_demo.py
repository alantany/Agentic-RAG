import streamlit as st
import importlib
import subprocess
import sys
import sqlite3
import json
from vector_store import VectorStore
import networkx as nx

# æ£€æŸ¥å¹¶å®‰è£…ä¾èµ–
def install_missing_packages():
    required_packages = {
        'openai': 'openai',
        'faiss': 'faiss-cpu',
        'sqlalchemy': 'sqlalchemy',
        'networkx': 'networkx',
        'sentence_transformers': 'sentence-transformers'
    }
    
    for package, pip_name in required_packages.items():
        if importlib.util.find_spec(package) is None:
            st.warning(f"æ­£åœ¨å®‰è£… {pip_name}...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", pip_name])
            st.success(f"{pip_name} å®‰è£…æˆåŠŸï¼")

install_missing_packages()

# åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
client = OpenAI(
    api_key="sk-2D0EZSwcWUcD4c2K59353b7214854bBd8f35Ac131564EfBa",
    base_url="https://free.gpt.ge/v1"
)

# åˆå§‹åŒ–å‘é‡å­˜å‚¨
vector_store = VectorStore()

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(page_title="Agentic RAG ç³»ç»Ÿ", layout="wide")

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []

def get_vector_search_results(query: str) -> list:
    try:
        results = vector_store.search(query)
        return [f"{item['title']}: {item['content']}" for item in results]
    except Exception as e:
        st.error(f"å‘é‡æœç´¢é”™è¯¯: {str(e)}")
        return []

def get_rdb_search_results(query: str) -> list:
    try:
        conn = sqlite3.connect('knowledge_base.db')
        cursor = conn.cursor()
        
        cursor.execute("""
        SELECT title, content FROM documents 
        WHERE title LIKE ? OR content LIKE ?
        LIMIT 3
        """, (f"%{query}%", f"%{query}%"))
        
        results = cursor.fetchall()
        conn.close()
        
        return [f"{title}: {content}" for title, content in results]
    except Exception as e:
        st.error(f"æ•°æ®åº“æœç´¢é”™è¯¯: {str(e)}")
        return []

def get_graph_search_results(query: str) -> list:
    try:
        G = nx.read_gexf("knowledge_graph.gexf")
        results = []
        
        for node in G.nodes(data=True):
            if query.lower() in node[1].get('content', '').lower():
                results.append(f"{node[0]}: {node[1].get('content', '')}")
        
        return results[:3]
    except Exception as e:
        st.error(f"å›¾æ•°æ®åº“æœç´¢é”™è¯¯: {str(e)}")
        return []

def get_llm_response(query: str, search_results: dict) -> str:
    try:
        # æ„å»ºæç¤ºè¯
        prompt = f"""
        æŸ¥è¯¢: {query}
        æœç´¢ç»“æœ:
        å‘é‡æœç´¢: {search_results['vector']}
        å…³ç³»æ•°æ®åº“: {search_results['rdb']}
        å›¾æ•°æ®åº“: {search_results['graph']}
        
        è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ç”Ÿæˆå›ç­”ã€‚
        """
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
    except Exception as e:
        st.error(f"LLMå“åº”é”™è¯¯: {str(e)}")
        return "æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ã€‚"

def process_medical_query(query: str):
    """å¤„ç†åŒ»ç–—æŸ¥è¯¢"""
    # å‘é‡æœç´¢æ‰¾ç›¸ä¼¼ç—…ä¾‹
    vector_results = vector_store.search(query)
    
    # å…³ç³»æ•°æ®åº“æŸ¥è¯¢å…·ä½“ä¿¡æ¯
    with sqlite3.connect('medical_records.db') as conn:
        cursor = conn.cursor()
        cursor.execute("""
            SELECT p.name, d.diagnosis 
            FROM patients p 
            JOIN diagnoses d ON p.id = d.patient_id 
            WHERE d.diagnosis LIKE ?
        """, (f"%{query}%",))
        sql_results = cursor.fetchall()
    
    # å›¾æ•°æ®åº“æŸ¥è¯¢å…³ç³»
    G = nx.read_gexf("medical_graph.gexf")
    graph_results = []
    for node in G.nodes(data=True):
        if query.lower() in str(node[1]).lower():
            neighbors = list(G.neighbors(node[0]))
            graph_results.append(f"{node[0]} ç›¸å…³: {neighbors}")
    
    return {
        "vector": vector_results,
        "sql": sql_results,
        "graph": graph_results
    }

# ä¸»ç•Œé¢
st.title("ğŸ¤– Agentic RAG ç³»ç»Ÿ")

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.header("ç³»ç»Ÿé…ç½®")
    search_methods = st.multiselect(
        "é€‰æ‹©æœç´¢æ–¹æ³•",
        ["å‘é‡æœç´¢", "å…³ç³»æ•°æ®åº“", "å›¾æ•°æ®åº“"],
        default=["å‘é‡æœç´¢", "å…³ç³»æ•°æ®åº“", "å›¾æ•°æ®åº“"]
    )

# ä¸»è¦å†…å®¹åŒºåŸŸ
query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š")

if st.button("æœç´¢å¹¶ç”Ÿæˆ"):
    with st.spinner("æ­£åœ¨å¤„ç†..."):
        # æ‰§è¡Œæœç´¢
        search_results = {
            "vector": get_vector_search_results(query) if "å‘é‡æœç´¢" in search_methods else [],
            "rdb": get_rdb_search_results(query) if "å…³ç³»æ•°æ®åº“" in search_methods else [],
            "graph": get_graph_search_results(query) if "å›¾æ•°æ®åº“" in search_methods else []
        }
        
        # è·å–LLMå“åº”
        response = get_llm_response(query, search_results)
        
        # æ›´æ–°å¯¹è¯å†å²
        st.session_state.chat_history.append({"query": query, "response": response})

# æ˜¾ç¤ºå¯¹è¯å†å²
st.subheader("å¯¹è¯å†å²")
for chat in st.session_state.chat_history:
    st.text_area("é—®é¢˜ï¼š", chat["query"], height=50, disabled=True)
    st.text_area("å›ç­”ï¼š", chat["response"], height=100, disabled=True)
    st.markdown("---") 