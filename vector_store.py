from pinecone import Pinecone, ServerlessSpec
from sentence_transformers import SentenceTransformer
import streamlit as st
import tiktoken
import traceback
import time
import jieba  # æ·»åŠ ä¸­æ–‡åˆ†è¯åº“
import networkx as nx
import json
from openai import OpenAI
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from config import get_pinecone_config, get_openai_client, get_sentence_transformer_config

# åˆå§‹åŒ– Pinecone
def init_pinecone():
    """åˆå§‹åŒ– Pinecone å®¢æˆ·ç«¯"""
    try:
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„Pineconeè®¾ç½®
        config = get_pinecone_config()
        pc = Pinecone(api_key=config["api_key"])
        
        index_name = config["index_name"]
        
        # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
        existing_indexes = pc.list_indexes()
        index_names = [index.name for index in existing_indexes]
        
        if index_name not in index_names:
            st.write("åˆ›å»ºæ–°çš„ Pinecone ç´¢å¼•...")
            pc.create_index(
                name=index_name,
                dimension=config["dimension"],
                metric=config["metric"],
                spec=ServerlessSpec(
                    cloud="aws",
                    region="us-east-1"
                )
            )
            st.write("âœ… æ–°ç´¢å¼•åˆ›å»ºæˆåŠŸ")
        else:
            st.write("âœ… ä½¿ç”¨ç°æœ‰çš„ Pinecone ç´¢å¼•")
        
        return pc.Index(index_name)
    except Exception as e:
        st.error(f"Pinecone åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return None

def get_embeddings(texts):
    """è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º"""
    try:
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„sentence transformerè®¾ç½®
        import os
        os.environ['HF_HUB_OFFLINE'] = '0'
        os.environ['TRANSFORMERS_OFFLINE'] = '0'
        
        st_config = get_sentence_transformer_config()
        model = SentenceTransformer(
            st_config["model_name"], 
            cache_folder=st_config["cache_folder"],
            device=st_config["device"]
        )
        
        # å¯¹ä¸­æ–‡æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†
        processed_texts = []
        for text in texts:
            # ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯
            words = jieba.cut(text)
            # å°†åˆ†è¯ç»“æœé‡æ–°ç»„åˆï¼Œç”¨ç©ºæ ¼è¿æ¥
            processed_text = " ".join(words)
            processed_texts.append(processed_text)
        
        # è·å–å‘é‡è¡¨ç¤º
        embeddings = model.encode(processed_texts)
        return embeddings
    except Exception as e:
        st.warning(f"æ— æ³•ä½¿ç”¨sentence-transformersæ¨¡å‹: {str(e)}")
        st.info("ä½¿ç”¨ç®€å•çš„æ–‡æœ¬å‘é‡åŒ–æ–¹æ³•ä½œä¸ºå¤‡é€‰...")
        
        # å¤‡é€‰æ–¹æ¡ˆï¼šä½¿ç”¨ç®€å•çš„TF-IDFå‘é‡åŒ–
        return get_simple_embeddings(texts)

def get_simple_embeddings(texts):
    """ä½¿ç”¨TF-IDFçš„ç®€å•å‘é‡åŒ–æ–¹æ³•"""
    try:
        # å¯¹ä¸­æ–‡æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†
        processed_texts = []
        for text in texts:
            # ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯
            words = jieba.cut(text)
            processed_text = " ".join(words)
            processed_texts.append(processed_text)
        
        # ä½¿ç”¨TF-IDFå‘é‡åŒ–
        vectorizer = TfidfVectorizer(max_features=384, stop_words=None)
        tfidf_matrix = vectorizer.fit_transform(processed_texts)
        
        # è½¬æ¢ä¸ºå¯†é›†çŸ©é˜µå¹¶ç¡®ä¿ç»´åº¦ä¸º384
        embeddings = tfidf_matrix.toarray()
        
        # å¦‚æœç‰¹å¾æ•°å°‘äº384ï¼Œç”¨é›¶å¡«å……
        if embeddings.shape[1] < 384:
            padding = np.zeros((embeddings.shape[0], 384 - embeddings.shape[1]))
            embeddings = np.hstack([embeddings, padding])
        elif embeddings.shape[1] > 384:
            # å¦‚æœç‰¹å¾æ•°å¤šäº384ï¼Œæˆªå–å‰384ä¸ª
            embeddings = embeddings[:, :384]
        
        return embeddings
    except Exception as e:
        st.error(f"ç®€å•å‘é‡åŒ–ä¹Ÿå¤±è´¥äº†: {str(e)}")
        # æœ€åçš„å¤‡é€‰æ–¹æ¡ˆï¼šè¿”å›éšæœºå‘é‡
        return np.random.random((len(texts), 384))

def vectorize_document(text: str, file_name: str = None):
    """å‘é‡åŒ–æ–‡æ¡£å¹¶å­˜å‚¨åˆ° Pinecone"""
    try:
        # æ–‡æœ¬åˆ†å—
        chunks = text_to_chunks(text)
        
        # è·å– embeddings
        embeddings = get_embeddings(chunks)
        
        # åˆå§‹åŒ– Pinecone
        index = init_pinecone()
        if not index:
            return None, None
        
        # ç”Ÿæˆå”¯ä¸€çš„æ–‡æ¡£IDå‰ç¼€ï¼ˆç¡®ä¿æ˜¯ASCIIå­—ç¬¦ï¼‰
        doc_id = f"doc_{int(time.time())}"  # ä¸ä½¿ç”¨æ–‡ä»¶åï¼Œæ”¹ç”¨æ—¶é—´æˆ³
        
        # ä¸Šä¼ å‘é‡åˆ° Pinecone
        vectors = []
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            vector = {
                'id': f"{doc_id}_chunk_{i}",  # ä½¿ç”¨æ—¶é—´æˆ³ä½œä¸ºIDå‰ç¼€
                'values': embedding.tolist(),
                'metadata': {
                    'text': chunk,
                    'original_file_name': file_name,  # åœ¨å…ƒæ•°æ®ä¸­ä¿å­˜åŸå§‹æ–‡ä»¶å
                    'chunk_index': i,
                    'timestamp': time.time()
                }
            }
            vectors.append(vector)
        
        # æ‰¹é‡ä¸Šä¼ 
        index.upsert(vectors=vectors)
        
        return chunks, index
    except Exception as e:
        st.error(f"å‘é‡åŒ–æ–‡æ¡£å¤±è´¥: {str(e)}")
        st.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        st.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        return None, None

def search_similar(query: str, index, chunks=None, top_k=3):
    """åœ¨ Pinecone ä¸­æœç´¢ç›¸ä¼¼å†…å®¹"""
    try:
        # è·å–æŸ¥è¯¢çš„ embedding
        query_embedding = get_embeddings([query])[0]
        
        # ç›´æ¥ä» Pinecone ä¸­æœç´¢ï¼Œä¸å†ä½¿ç”¨æœ¬åœ°çš„ chunks
        results = index.query(
            vector=query_embedding.tolist(),
            top_k=top_k,
            include_metadata=True
        )
        
        # åªè¿”å›ç›¸ä¼¼åº¦è¾ƒé«˜çš„ç›¸å…³æ–‡æœ¬
        matched_texts = []
        for match in results['matches']:
            if match['score'] >= 0.3:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                text = match['metadata']['text']
                matched_texts.append(text)
        
        return matched_texts
    except Exception as e:
        st.error(f"æœç´¢å¤±è´¥: {str(e)}")
        st.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        st.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        return []

def get_vector_search_results(query: str) -> list:
    """ä» Pinecone ä¸­æœç´¢ç›¸å…³ä¿¡æ¯"""
    try:
        # åˆå§‹åŒ– Pinecone ç´¢å¼•
        index = init_pinecone()
        if not index:
            return []
        
        # ä»æŸ¥è¯¢ä¸­æå–æ‚£è€…å§“å
        import re
        # æ›´å®½æ¾çš„æ‚£è€…å§“ååŒ¹é…ï¼ŒåŒ…æ‹¬å¸¸è§å§“æ°+æŸæŸçš„æ¨¡å¼
        common_surnames = "æç‹å¼ åˆ˜é™ˆæ¨é»„å‘¨å´é©¬è’²èµµé’±å­™æœ±èƒ¡éƒ­ä½•é«˜æ—ç½—éƒ‘æ¢è°¢å®‹å”è®¸é‚“å†¯éŸ©æ›¹æ›¾å½­è§è”¡æ½˜ç”°è‘£è¢äºä½™å¶è’‹æœè‹é­ç¨‹å•ä¸æ²ˆä»»å§šå¢å‚…é’Ÿå§œå´”è°­å»–èŒƒæ±ªé™†é‡‘çŸ³æˆ´è´¾éŸ¦å¤é‚±æ–¹ä¾¯é‚¹ç†Šå­Ÿç§¦ç™½æ±Ÿé˜è–›å°¹æ®µé›·é»å²é¾™é™¶è´ºé¡¾æ¯›éƒé¾šé‚µä¸‡é’±ä¸¥è¦ƒæ­¦æˆ´è«å­”å‘æ±¤"
        pattern = f'([{common_surnames}])æŸæŸ'
        patient_name_match = re.search(pattern, query)
        if not patient_name_match:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šæ‚£è€…å§“åï¼Œå°è¯•é€šç”¨æœç´¢
            st.info("æœªè¯†åˆ«åˆ°ç‰¹å®šæ‚£è€…å§“åï¼Œè¿›è¡Œé€šç”¨å‘é‡æœç´¢...")
            patient_name = None
        else:
            patient_name = patient_name_match.group(1) + "æŸæŸ"
            st.write(f"æŸ¥è¯¢æ‚£è€…ï¼š{patient_name}")
        
        # è·å–æŸ¥è¯¢çš„ embedding
        query_embedding = get_embeddings([query])[0]
        
        # åœ¨ Pinecone ä¸­æœç´¢ï¼Œä¸ä½¿ç”¨è¿‡æ»¤å™¨
        results = index.query(
            vector=query_embedding.tolist(),
            top_k=50,  # å¢åŠ è¿”å›ç»“æœæ•°é‡
            include_metadata=True
        )
        
        # åœ¨ç»“æœä¸­è¿‡æ»¤å’Œå¤„ç†åŒ¹é…ç»“æœ
        matched_texts = []
        if results['matches']:
            for match in results['matches']:
                file_name = match['metadata'].get('original_file_name', '')
                score = match['score']
                text = match['metadata']['text']
                
                # å¦‚æœæŒ‡å®šäº†æ‚£è€…å§“åï¼Œä¼˜å…ˆåŒ¹é…è¯¥æ‚£è€…çš„æ–‡ä»¶
                if patient_name:
                    if patient_name in file_name:
                        st.write(f"æ‰¾åˆ°æ‚£è€…åŒ¹é…ï¼š")
                        st.write(f"- æ–‡ä»¶å: {file_name}")
                        st.write(f"- ç›¸ä¼¼åº¦: {score:.2f}")
                        
                        if score >= 0.01:  # ä¿æŒä¸€ä¸ªæœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼
                            matched_texts.append(f"[{file_name}] (ç›¸ä¼¼åº¦: {score:.2f}): {text}")
                else:
                    # å¦‚æœæ²¡æœ‰æŒ‡å®šæ‚£è€…å§“åï¼Œè¿”å›æ‰€æœ‰ç›¸å…³åº¦è¾ƒé«˜çš„ç»“æœ
                    if score >= 0.3:  # æé«˜é€šç”¨æœç´¢çš„ç›¸ä¼¼åº¦é˜ˆå€¼
                        st.write(f"æ‰¾åˆ°ç›¸å…³åŒ¹é…ï¼š")
                        st.write(f"- æ–‡ä»¶å: {file_name}")
                        st.write(f"- ç›¸ä¼¼åº¦: {score:.2f}")
                        matched_texts.append(f"[{file_name}] (ç›¸ä¼¼åº¦: {score:.2f}): {text}")
                
                # é™åˆ¶è¿”å›ç»“æœæ•°é‡
                if len(matched_texts) >= 5:
                    break
        
        return matched_texts
    except Exception as e:
        st.error(f"å‘é‡æœç´¢é”™è¯¯: {str(e)}")
        return []

def text_to_chunks(text: str, chunk_size: int = 100000):
    """å°†æ–‡æœ¬åˆ†å‰²æˆå°å—"""
    words = text.split()
    chunks = []
    current_chunk = []
    current_size = 0
    
    for word in words:
        current_chunk.append(word)
        current_size += len(word) + 1  # +1 for space
        
        if current_size >= chunk_size:
            chunks.append(' '.join(current_chunk))
            current_chunk = []
            current_size = 0
    
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    
    return chunks

def clean_vector_store():
    """æ¸…ç†å‘é‡æ•°æ®åº“"""
    try:
        index = init_pinecone()
        if index:
            # åˆ é™¤æ‰€æœ‰å‘é‡ï¼ˆæ–°ç‰ˆæœ¬APIï¼‰
            try:
                index.delete(delete_all=True)
            except Exception as delete_error:
                # å¦‚æœdelete_allä¸æ”¯æŒï¼Œå°è¯•è·å–æ‰€æœ‰å‘é‡IDå¹¶é€ä¸ªåˆ é™¤
                st.warning("å°è¯•ä½¿ç”¨æ›¿ä»£æ–¹æ³•æ¸…ç†å‘é‡æ•°æ®åº“...")
                stats = index.describe_index_stats()
                if stats.total_vector_count > 0:
                    # ç”±äºæ–°ç‰ˆæœ¬å¯èƒ½ä¸æ”¯æŒç›´æ¥åˆ é™¤æ‰€æœ‰å‘é‡ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ–°çš„ç´¢å¼•æ¥æ›¿ä»£
                    st.warning("å½“å‰ç´¢å¼•åŒ…å«æ•°æ®ï¼Œå»ºè®®æ‰‹åŠ¨æ¸…ç†æˆ–é‡æ–°åˆ›å»ºç´¢å¼•")
            st.success("âœ… Pinecone å‘é‡æ•°æ®åº“å·²æ¸…ç©º")
            return True
    except Exception as e:
        st.error(f"æ¸…ç† Pinecone æ•°æ®åº“é”™è¯¯: {str(e)}")
        return False

# æ·»åŠ  num_tokens_from_string å‡½æ•°
def num_tokens_from_string(string: str, encoding_name: str = "cl100k_base") -> int:
    """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens

def get_graph_search_results(query: str) -> list:
    """ä»å›¾æ•°æ®åº“ä¸­æœç´¢ç›¸å…³ä¿¡æ¯"""
    try:
        # ä½¿ç”¨LLMç”ŸæˆæŸ¥è¯¢æ¡ä»¶
        query_obj = generate_graph_query(query)
        if not query_obj:
            return []
        
        G = nx.read_gexf("medical_graph.gexf")
        results = []
        
        # æ ¹æ®æŸ¥è¯¢æ¡ä»¶æ‰§è¡Œæœç´¢
        start_nodes = [node for node, data in G.nodes(data=True)
                      if data.get('type') == query_obj["start_node"]["type"] and 
                         node == query_obj["start_node"]["name"]]
        
        for start_node in start_nodes:
            # è·å–æ‰€æœ‰é‚»å±…èŠ‚ç‚¹
            for neighbor in G.neighbors(start_node):
                edge_data = G.get_edge_data(start_node, neighbor)
                neighbor_data = G.nodes[neighbor]
                
                # æ£€æŸ¥å…³ç³»ç±»å‹å’Œç»ˆç‚¹èŠ‚ç‚¹ç±»å‹æ˜¯å¦åŒ¹é…
                if (edge_data.get("relationship") == query_obj["relationship"] and
                    neighbor_data.get('type') == query_obj["end_node"]["type"]):
                    
                    # æ„å»ºç»“æœ
                    result = []
                    for attr in query_obj["return"]:
                        node_type, attr_name = attr.split(".")
                        if node_type == "end_node":
                            # å¦‚æœæ˜¯è¯Šæ–­èŠ‚ç‚¹ï¼Œç›´æ¥ä½¿ç”¨èŠ‚ç‚¹çš„åç§°ä½œä¸ºè¯Šæ–­å†…å®¹
                            if neighbor_data.get('type') in ['admission_diagnosis', 'discharge_diagnosis']:
                                result.append(f"{attr_name}: {neighbor}")
                            else:
                                result.append(f"{attr_name}: {neighbor_data.get(attr_name, '')}")
                    
                    if result:  # åªæ·»åŠ æœ‰å†…å®¹çš„ç»“æœ
                        results.append(f"{start_node} -> {edge_data.get('relationship')} -> {' | '.join(result)}")
        
        return results
    except Exception as e:
        st.error(f"å›¾æ•°æ®åº“æœç´¢é”™è¯¯: {str(e)}")
        st.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        st.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        return []

def generate_graph_query(query: str) -> dict:
    """ä½¿ç”¨LLMç”Ÿæˆå›¾æ•°æ®åº“æŸ¥è¯¢æ¡ä»¶"""
    try:
        st.write("å¼€å§‹åˆ›å»ºOpenAIå®¢æˆ·ç«¯...")
        client = OpenAI(
            api_key="sk-1pUmQlsIkgla3CuvKTgCrzDZ3r0pBxO608YJvIHCN18lvOrn",
            base_url="https://api.chatanywhere.tech/v1",
            timeout=60
        )
        st.write("âœ… OpenAIå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        
        # è¯»å–å›¾æ•°æ®åº“çš„ç»“æ„ä¿¡æ¯
        G = nx.read_gexf("medical_graph.gexf")
        
        # è·å–å›¾çš„åŸºæœ¬ä¿¡æ¯
        graph_info = {
            "node_types": list(set(nx.get_node_attributes(G, 'type').values())),
            "relationships": list(set(nx.get_edge_attributes(G, 'relationship').values())),
            "nodes_sample": {
                node: data for node, data in list(G.nodes(data=True))[:5]
            },
            "edges_sample": {
                f"{u}->{v}": data for u, v, data in list(G.edges(data=True))[:5]
            }
        }
        
        prompt = f"""è¯·æ ¹æ®é—®é¢˜å’Œå›¾æ•°æ®åº“ç»“æ„ç”Ÿæˆå›¾æ•°æ®åº“æŸ¥è¯¢æ¡ä»¶ã€‚

å›¾æ•°æ®åº“ç»“æ„ï¼š
èŠ‚ç‚¹ç±»å‹: {graph_info["node_types"]}
å…³ç³»ç±»å‹: {graph_info["relationships"]}
èŠ‚ç‚¹ç¤ºä¾‹: {json.dumps(graph_info["nodes_sample"], ensure_ascii=False, indent=2)}
å…³ç³»ç¤ºä¾‹: {json.dumps(graph_info["edges_sample"], ensure_ascii=False, indent=2)}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·ç”Ÿæˆä¸€ä¸ªåŒ…å«æŸ¥è¯¢æ¡ä»¶çš„å­—å…¸ï¼Œç¤ºä¾‹æ ¼å¼ï¼š

1. æŸ¥è¯¢æ‚£è€…çš„ä¸»è¯‰ï¼š
{{
    "start_node": {{"type": "patient", "name": "ä»é—®é¢˜ä¸­æå–çš„æ‚£è€…å§“å"}},
    "relationship": "complains_of",
    "end_node": {{"type": "chief_complaint"}},
    "return": ["end_node.content"]
}}

æ³¨æ„ï¼š
1. ä»ç”¨æˆ·é—®é¢˜ä¸­æå–æ­£ç¡®çš„æ‚£è€…å§“å
2. ä½¿ç”¨æ­£ç¡®çš„èŠ‚ç‚¹ç±»å‹å’Œå…³ç³»ç±»å‹
3. ä½¿ç”¨æ­£ç¡®çš„å±æ€§åç§°
4. æŒ‡å®šè¦è¿”å›çš„å…·ä½“å±æ€§

è¯·ç›´æ¥è¿”å›æŸ¥è¯¢æ¡ä»¶çš„JSONå­—ç¬¦ä¸²ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ã€‚"""

        st.write("ğŸ”„ æ­£åœ¨è°ƒç”¨OpenAI API...")
        response = client.chat.completions.create(
            model="gpt-4o-mini-2024-07-18",
            messages=[
                {
                    "role": "system", 
                    "content": "ä½ æ˜¯ä¸€ä¸ªå›¾æ•°æ®åº“æŸ¥è¯¢ä¸“å®¶ã€‚è¯·æ ¹æ®å®é™…çš„å›¾æ•°æ®åº“ç»“æ„ç”Ÿæˆç²¾ç¡®çš„æŸ¥è¯¢æ¡ä»¶ã€‚"
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            temperature=0.1
        )
        st.write("âœ… OpenAI APIè°ƒç”¨æˆåŠŸ")
        
        # è·å–å“åº”æ–‡æœ¬å¹¶æ¸…ç†
        query_str = response.choices[0].message.content.strip()
        st.write("åŸå§‹å“åº”æ–‡æœ¬ï¼š", query_str)
        
        if query_str.startswith('```json'):
            query_str = query_str[7:]
        if query_str.endswith('```'):
            query_str = query_str[:-3]
        query_str = query_str.strip()
        
        st.write("æ¸…ç†åçš„JSONå­—ç¬¦ä¸²ï¼š", query_str)
        
        # æ˜¾ç¤ºç”Ÿæˆçš„æŸ¥è¯¢æ¡ä»¶
        st.write("ç”Ÿæˆçš„å›¾æ•°æ®åº“æŸ¥è¯¢æ¡ä»¶ï¼š")
        st.code(query_str, language="json")
        
        return json.loads(query_str)
        
    except Exception as e:
        st.error(f"ç”Ÿæˆå›¾æ•°æ®åº“æŸ¥è¯¢æ¡ä»¶é”™è¯¯: {str(e)}")
        st.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        st.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        return None
