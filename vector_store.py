import pinecone
from sentence_transformers import SentenceTransformer
import streamlit as st
import tiktoken
import traceback
import time
import jieba  # æ·»åŠ ä¸­æ–‡åˆ†è¯åº“
import networkx as nx
import json
from openai import OpenAI

# åˆå§‹åŒ– Pinecone
def init_pinecone():
    """åˆå§‹åŒ– Pinecone å®¢æˆ·ç«¯"""
    try:
        pinecone.init(
            api_key="e5b7d591-d2c7-411a-9b9b-c52d17934415",
            environment="gcp-starter"
        )
        
        index_name = "medical-records"
        
        # åªåœ¨ç´¢å¼•ä¸å­˜åœ¨æ—¶åˆ›å»ºæ–°ç´¢å¼•
        if index_name not in pinecone.list_indexes():
            st.write("åˆ›å»ºæ–°çš„ Pinecone ç´¢å¼•...")
            dimension = 384  # all-MiniLM-L6-v2 çš„ç»´åº¦
            pinecone.create_index(
                name=index_name,
                dimension=dimension,
                metric="cosine"
            )
            st.write("âœ… æ–°ç´¢å¼•åˆ›å»ºæˆåŠŸ")
        else:
            st.write("âœ… ä½¿ç”¨ç°æœ‰çš„ Pinecone ç´¢å¼•")
        
        return pinecone.Index(index_name)
    except Exception as e:
        st.error(f"Pinecone åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return None

def get_embeddings(texts):
    """è·å–æ–‡æœ¬çš„å‘é‡è¡¨ç¤º"""
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    # å¯¹ä¸­æ–‡æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†
    processed_texts = []
    for text in texts:
        # ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯
        words = jieba.cut(text)
        # å°†åˆ†è¯ç»“æœé‡æ–°ç»„åˆï¼Œç”¨ç©ºæ ¼è¿æ¥
        processed_text = " ".join(words)
        processed_texts.append(processed_text)
    
    # è·å–å‘é‡è¡¨ç¤º
    embeddings = model.encode(processed_texts)
    return embeddings

def vectorize_document(text: str, file_name: str = None):
    """å‘é‡åŒ–æ–‡æ¡£å¹¶å­˜å‚¨åˆ° Pinecone"""
    try:
        # æ–‡æœ¬åˆ†å—
        chunks = text_to_chunks(text)
        
        # è·å– embeddings
        embeddings = get_embeddings(chunks)
        
        # åˆå§‹åŒ– Pinecone
        index = init_pinecone()
        if not index:
            return None, None
        
        # ç”Ÿæˆå”¯ä¸€çš„æ–‡æ¡£IDå‰ç¼€ï¼ˆç¡®ä¿æ˜¯ASCIIå­—ç¬¦ï¼‰
        doc_id = f"doc_{int(time.time())}"  # ä¸ä½¿ç”¨æ–‡ä»¶åï¼Œæ”¹ç”¨æ—¶é—´æˆ³
        
        # ä¸Šä¼ å‘é‡åˆ° Pinecone
        vectors = []
        for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
            vector = {
                'id': f"{doc_id}_chunk_{i}",  # ä½¿ç”¨æ—¶é—´æˆ³ä½œä¸ºIDå‰ç¼€
                'values': embedding.tolist(),
                'metadata': {
                    'text': chunk,
                    'original_file_name': file_name,  # åœ¨å…ƒæ•°æ®ä¸­ä¿å­˜åŸå§‹æ–‡ä»¶å
                    'chunk_index': i,
                    'timestamp': time.time()
                }
            }
            vectors.append(vector)
        
        # æ‰¹é‡ä¸Šä¼ 
        index.upsert(vectors=vectors)
        
        return chunks, index
    except Exception as e:
        st.error(f"å‘é‡åŒ–æ–‡æ¡£å¤±è´¥: {str(e)}")
        st.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        st.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        return None, None

def search_similar(query: str, index, chunks=None, top_k=3):
    """åœ¨ Pinecone ä¸­æœç´¢ç›¸ä¼¼å†…å®¹"""
    try:
        # è·å–æŸ¥è¯¢çš„ embedding
        query_embedding = get_embeddings([query])[0]
        
        # ç›´æ¥ä» Pinecone ä¸­æœç´¢ï¼Œä¸å†ä½¿ç”¨æœ¬åœ°çš„ chunks
        results = index.query(
            vector=query_embedding.tolist(),
            top_k=top_k,
            include_metadata=True
        )
        
        # åªè¿”å›ç›¸ä¼¼åº¦è¾ƒé«˜çš„ç›¸å…³æ–‡æœ¬
        matched_texts = []
        for match in results['matches']:
            if match['score'] >= 0.3:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                text = match['metadata']['text']
                matched_texts.append(text)
        
        return matched_texts
    except Exception as e:
        st.error(f"æœç´¢å¤±è´¥: {str(e)}")
        st.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        st.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        return []

def get_vector_search_results(query: str) -> list:
    """ä» Pinecone ä¸­æœç´¢ç›¸å…³ä¿¡æ¯"""
    try:
        # åˆå§‹åŒ– Pinecone ç´¢å¼•
        index = init_pinecone()
        if not index:
            return []
        
        # ä»æŸ¥è¯¢ä¸­æå–æ‚£è€…å§“å
        import re
        patient_name_match = re.search(r'([æ|ç‹|å¼ |åˆ˜|é™ˆ|æ¨|é»„|å‘¨|å´|é©¬|è’²]æŸæŸ)', query)
        if not patient_name_match:
            st.warning("æœªèƒ½ä»é—®é¢˜ä¸­è¯†åˆ«å‡ºæ‚£è€…å§“å")
            return []
        
        patient_name = patient_name_match.group(1)
        st.write(f"æŸ¥è¯¢æ‚£è€…ï¼š{patient_name}")
        
        # è·å–æŸ¥è¯¢çš„ embedding
        query_embedding = get_embeddings([query])[0]
        
        # åœ¨ Pinecone ä¸­æœç´¢ï¼Œä¸ä½¿ç”¨è¿‡æ»¤å™¨
        results = index.query(
            vector=query_embedding.tolist(),
            top_k=50,  # å¢åŠ è¿”å›ç»“æœæ•°é‡
            include_metadata=True
        )
        
        # åœ¨ç»“æœä¸­æ‰‹åŠ¨è¿‡æ»¤æ‚£è€…
        matched_texts = []
        if results['matches']:
            for match in results['matches']:
                # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åŒ…å«æ‚£è€…å§“å
                file_name = match['metadata'].get('original_file_name', '')
                if patient_name in file_name:
                    score = match['score']
                    text = match['metadata']['text']
                    
                    # æ˜¾ç¤ºåŒ¹é…ä¿¡æ¯
                    st.write(f"æ‰¾åˆ°åŒ¹é…ï¼š")
                    st.write(f"- æ–‡ä»¶å: {file_name}")
                    st.write(f"- ç›¸ä¼¼åº¦: {score:.2f}")
                    
                    # åªè¿”å›ç›¸å…³çš„æ–‡æ¡£
                    if score >= 0.01:  # ä¿æŒä¸€ä¸ªæœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼
                        matched_texts.append(f"[{file_name}] (ç›¸ä¼¼åº¦: {score:.2f}): {text}")
                        break  # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…å°±é€€å‡º
        
        return matched_texts
    except Exception as e:
        st.error(f"å‘é‡æœç´¢é”™è¯¯: {str(e)}")
        return []

def text_to_chunks(text: str, chunk_size: int = 100000):
    """å°†æ–‡æœ¬åˆ†å‰²æˆå°å—"""
    words = text.split()
    chunks = []
    current_chunk = []
    current_size = 0
    
    for word in words:
        current_chunk.append(word)
        current_size += len(word) + 1  # +1 for space
        
        if current_size >= chunk_size:
            chunks.append(' '.join(current_chunk))
            current_chunk = []
            current_size = 0
    
    if current_chunk:
        chunks.append(' '.join(current_chunk))
    
    return chunks

def clean_vector_store():
    """æ¸…ç†å‘é‡æ•°æ®åº“"""
    try:
        index = init_pinecone()
        if index:
            # åˆ é™¤æ‰€æœ‰å‘é‡
            index.delete(delete_all=True)
            st.success("âœ… Pinecone å‘é‡æ•°æ®åº“å·²æ¸…ç©º")
            return True
    except Exception as e:
        st.error(f"æ¸…ç† Pinecone æ•°æ®åº“é”™è¯¯: {str(e)}")
        return False

# æ·»åŠ  num_tokens_from_string å‡½æ•°
def num_tokens_from_string(string: str, encoding_name: str = "cl100k_base") -> int:
    """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
    encoding = tiktoken.get_encoding(encoding_name)
    num_tokens = len(encoding.encode(string))
    return num_tokens

def get_graph_search_results(query: str) -> list:
    """ä»å›¾æ•°æ®åº“ä¸­æœç´¢ç›¸å…³ä¿¡æ¯"""
    try:
        # ä½¿ç”¨LLMç”ŸæˆæŸ¥è¯¢æ¡ä»¶
        query_obj = generate_graph_query(query)
        if not query_obj:
            return []
        
        G = nx.read_gexf("medical_graph.gexf")
        results = []
        
        # æ ¹æ®æŸ¥è¯¢æ¡ä»¶æ‰§è¡Œæœç´¢
        start_nodes = [node for node, data in G.nodes(data=True)
                      if data.get('type') == query_obj["start_node"]["type"] and 
                         node == query_obj["start_node"]["name"]]
        
        for start_node in start_nodes:
            # è·å–æ‰€æœ‰é‚»å±…èŠ‚ç‚¹
            for neighbor in G.neighbors(start_node):
                edge_data = G.get_edge_data(start_node, neighbor)
                neighbor_data = G.nodes[neighbor]
                
                # æ£€æŸ¥å…³ç³»ç±»å‹å’Œç»ˆç‚¹èŠ‚ç‚¹ç±»å‹æ˜¯å¦åŒ¹é…
                if (edge_data.get("relationship") == query_obj["relationship"] and
                    neighbor_data.get('type') == query_obj["end_node"]["type"]):
                    
                    # æ„å»ºç»“æœ
                    result = []
                    for attr in query_obj["return"]:
                        node_type, attr_name = attr.split(".")
                        if node_type == "end_node":
                            # å¦‚æœæ˜¯è¯Šæ–­èŠ‚ç‚¹ï¼Œç›´æ¥ä½¿ç”¨èŠ‚ç‚¹çš„åç§°ä½œä¸ºè¯Šæ–­å†…å®¹
                            if neighbor_data.get('type') in ['admission_diagnosis', 'discharge_diagnosis']:
                                result.append(f"{attr_name}: {neighbor}")
                            else:
                                result.append(f"{attr_name}: {neighbor_data.get(attr_name, '')}")
                    
                    if result:  # åªæ·»åŠ æœ‰å†…å®¹çš„ç»“æœ
                        results.append(f"{start_node} -> {edge_data.get('relationship')} -> {' | '.join(result)}")
        
        return results
    except Exception as e:
        st.error(f"å›¾æ•°æ®åº“æœç´¢é”™è¯¯: {str(e)}")
        st.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        st.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        return []

def generate_graph_query(query: str) -> dict:
    """ä½¿ç”¨LLMç”Ÿæˆå›¾æ•°æ®åº“æŸ¥è¯¢æ¡ä»¶"""
    try:
        st.write("å¼€å§‹åˆ›å»ºOpenAIå®¢æˆ·ç«¯...")
        client = OpenAI(
            api_key="sk-1pUmQlsIkgla3CuvKTgCrzDZ3r0pBxO608YJvIHCN18lvOrn",
            base_url="https://api.chatanywhere.tech/v1",
            timeout=60
        )
        st.write("âœ… OpenAIå®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
        
        # è¯»å–å›¾æ•°æ®åº“çš„ç»“æ„ä¿¡æ¯
        G = nx.read_gexf("medical_graph.gexf")
        
        # è·å–å›¾çš„åŸºæœ¬ä¿¡æ¯
        graph_info = {
            "node_types": list(set(nx.get_node_attributes(G, 'type').values())),
            "relationships": list(set(nx.get_edge_attributes(G, 'relationship').values())),
            "nodes_sample": {
                node: data for node, data in list(G.nodes(data=True))[:5]
            },
            "edges_sample": {
                f"{u}->{v}": data for u, v, data in list(G.edges(data=True))[:5]
            }
        }
        
        prompt = f"""è¯·æ ¹æ®é—®é¢˜å’Œå›¾æ•°æ®åº“ç»“æ„ç”Ÿæˆå›¾æ•°æ®åº“æŸ¥è¯¢æ¡ä»¶ã€‚

å›¾æ•°æ®åº“ç»“æ„ï¼š
èŠ‚ç‚¹ç±»å‹: {graph_info["node_types"]}
å…³ç³»ç±»å‹: {graph_info["relationships"]}
èŠ‚ç‚¹ç¤ºä¾‹: {json.dumps(graph_info["nodes_sample"], ensure_ascii=False, indent=2)}
å…³ç³»ç¤ºä¾‹: {json.dumps(graph_info["edges_sample"], ensure_ascii=False, indent=2)}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·ç”Ÿæˆä¸€ä¸ªåŒ…å«æŸ¥è¯¢æ¡ä»¶çš„å­—å…¸ï¼Œç¤ºä¾‹æ ¼å¼ï¼š

1. æŸ¥è¯¢æ‚£è€…çš„ä¸»è¯‰ï¼š
{{
    "start_node": {{"type": "patient", "name": "ä»é—®é¢˜ä¸­æå–çš„æ‚£è€…å§“å"}},
    "relationship": "complains_of",
    "end_node": {{"type": "chief_complaint"}},
    "return": ["end_node.content"]
}}

æ³¨æ„ï¼š
1. ä»ç”¨æˆ·é—®é¢˜ä¸­æå–æ­£ç¡®çš„æ‚£è€…å§“å
2. ä½¿ç”¨æ­£ç¡®çš„èŠ‚ç‚¹ç±»å‹å’Œå…³ç³»ç±»å‹
3. ä½¿ç”¨æ­£ç¡®çš„å±æ€§åç§°
4. æŒ‡å®šè¦è¿”å›çš„å…·ä½“å±æ€§

è¯·ç›´æ¥è¿”å›æŸ¥è¯¢æ¡ä»¶çš„JSONå­—ç¬¦ä¸²ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ã€‚"""

        st.write("ğŸ”„ æ­£åœ¨è°ƒç”¨OpenAI API...")
        response = client.chat.completions.create(
            model="gpt-4o-mini-2024-07-18",
            messages=[
                {
                    "role": "system", 
                    "content": "ä½ æ˜¯ä¸€ä¸ªå›¾æ•°æ®åº“æŸ¥è¯¢ä¸“å®¶ã€‚è¯·æ ¹æ®å®é™…çš„å›¾æ•°æ®åº“ç»“æ„ç”Ÿæˆç²¾ç¡®çš„æŸ¥è¯¢æ¡ä»¶ã€‚"
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            temperature=0.1
        )
        st.write("âœ… OpenAI APIè°ƒç”¨æˆåŠŸ")
        
        # è·å–å“åº”æ–‡æœ¬å¹¶æ¸…ç†
        query_str = response.choices[0].message.content.strip()
        st.write("åŸå§‹å“åº”æ–‡æœ¬ï¼š", query_str)
        
        if query_str.startswith('```json'):
            query_str = query_str[7:]
        if query_str.endswith('```'):
            query_str = query_str[:-3]
        query_str = query_str.strip()
        
        st.write("æ¸…ç†åçš„JSONå­—ç¬¦ä¸²ï¼š", query_str)
        
        # æ˜¾ç¤ºç”Ÿæˆçš„æŸ¥è¯¢æ¡ä»¶
        st.write("ç”Ÿæˆçš„å›¾æ•°æ®åº“æŸ¥è¯¢æ¡ä»¶ï¼š")
        st.code(query_str, language="json")
        
        return json.loads(query_str)
        
    except Exception as e:
        st.error(f"ç”Ÿæˆå›¾æ•°æ®åº“æŸ¥è¯¢æ¡ä»¶é”™è¯¯: {str(e)}")
        st.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        st.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
        return None
